# Configuration File for Qwen 14B Fine-tuning
# Edit these values to customize your training

[model]
# Base model to fine-tune
# Options: Qwen/Qwen2.5-14B, Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2-14B, etc.
model_name = Qwen/Qwen2.5-14B

# Maximum sequence length
max_seq_length = 2048

# Use 4-bit quantization for training
load_in_4bit = true

[lora]
# LoRA rank (higher = more parameters, better quality, more memory)
# Recommended: 8-32
r = 16

# LoRA alpha (usually same as rank)
alpha = 16

# LoRA dropout (0 = no dropout)
dropout = 0

# Target modules for LoRA
# Don't change unless you know what you're doing
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

[training]
# Output directory
output_dir = ./qwen14b_finetuned

# Dataset path
dataset_path = dataset.json

# Batch size per device
# Reduce if you run out of memory
batch_size = 2

# Gradient accumulation steps
# Effective batch size = batch_size * gradient_accumulation_steps
gradient_accumulation_steps = 4

# Number of training epochs
num_train_epochs = 3

# Learning rate
learning_rate = 2e-4

# Warmup steps
warmup_steps = 5

# Logging frequency
logging_steps = 10

# Save checkpoint frequency
save_steps = 100

# Maximum number of checkpoints to keep
save_total_limit = 3

[export]
# GGUF quantization methods to export
# Options: q2_k, q3_k_s, q3_k_m, q3_k_l, q4_0, q4_1, q4_k_s, q4_k_m, q5_0, q5_1, q5_k_s, q5_k_m, q6_k, q8_0, f16, f32
quantization_methods = ["q4_k_m", "q5_k_m", "q8_0"]

[inference]
# Default inference parameters
temperature = 0.7
top_p = 0.9
top_k = 40
max_new_tokens = 512
